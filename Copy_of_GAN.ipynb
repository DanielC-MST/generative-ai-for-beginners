{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanielC-MST/generative-ai-for-beginners/blob/main/Copy_of_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "enMYVyg399iY"
      },
      "source": [
        "```{note}\n",
        "If running in Colab, think of changing the runtime type before starting, in\n",
        "order  to have access to GPU ressources: Runtime->Change Runtime Type, then\n",
        "chose GPU for hardware accelerator.\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btxp4h4Q99iZ"
      },
      "source": [
        "In this lab we will focus on **image synthesis**, in particular to\n",
        "synthesize **T2-weighted MRI** from **T1-weighted MRI**.\n",
        "\n",
        "We will investigate three approaches to do so:\n",
        "\n",
        "1. First, we will train a generator (or encoder-decoder).\n",
        "2. Then, we will train a conditional generative adversarial network (cGAN).\n",
        "3. Finally, we will train a cycle generative adversarial network (CycleGAN).\n",
        "\n",
        "We will evaluate the quality of the generated images using several metrics.\n",
        "\n",
        "We will use the [IXI dataset](https://brain-development.org/ixi-dataset/)\n",
        "to have access to **paired T1-w and T2-w images**.\n",
        "Before creating and training the different neural networks,\n",
        "we will:\n",
        "\n",
        "1. fetch the dataset,\n",
        "2. have a look at it to see what the task looks like, and\n",
        "3. illustrate how to easily access the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bs3YJ6fQ99iZ"
      },
      "source": [
        "# 0. Fetching the dataset\n",
        "\n",
        "The dataset can be found on this\n",
        "[server](https://aramislab.paris.inria.fr/files/data/databases/DL4MI/IXI-dataset.tar.gz)\n",
        "and alternatively in the following\n",
        "[GitHub repository](https://github.com/Easternwen/IXI-dataset).\n",
        "In the `size64` folder, there are 1154 files: 2 images for 577 subjects.\n",
        "The size of each image is (64, 64).\n",
        "\n",
        "Let's download the file and have a look at the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zm_Ja67y99ia",
        "outputId": "df1ceb80-4cb2-42dc-cd47-3a7a6f240f37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'IXI-dataset'...\n",
            "remote: Enumerating objects: 1157, done.\u001b[K\n",
            "remote: Counting objects: 100% (1157/1157), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1156/1156), done.\u001b[K\n",
            "remote: Total 1157 (delta 0), reused 1157 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (1157/1157), 24.99 MiB | 10.95 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "# Get the dataset from the server\n",
        "! git clone https://github.com/Easternwen/IXI-dataset.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWm3M0DQ99ia"
      },
      "source": [
        "The dataset used in this lab is composed of preprocessed images from the\n",
        "[IXI dataset](https://brain-development.org/ixi-dataset/). Two different\n",
        "structural MRI modalities are comprised in this dataset:\n",
        "\n",
        "- T1 weighted images\n",
        "\n",
        "- T2 weighted images\n",
        "\n",
        "These modalities do not highlight the same tissues: for example the CSF\n",
        "voxels are cancelled in T1 weighted imaging whereas they are highlighted by\n",
        "the T2 weighted imaging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBzkQkWw99ia",
        "outputId": "a08341b2-fc04-4189-afa5-5e28ac788fd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-117bfebed12f>:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  plt.imshow(np.swapaxes(torch.load(os.path.join(root, 'sub-IXI002 - T1.pt')), 0, 1),\n",
            "<ipython-input-2-117bfebed12f>:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  plt.imshow(np.swapaxes(torch.load(os.path.join(root, 'sub-IXI002 - T2.pt')), 0, 1),\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 900x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "\n",
        "\n",
        "root = \"./IXI-dataset/size64/\"\n",
        "\n",
        "plt.figure(figsize=(9, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(np.swapaxes(torch.load(os.path.join(root, 'sub-IXI002 - T1.pt')), 0, 1),\n",
        "           cmap='gray', origin='lower')\n",
        "plt.title(\"T1 slice for subject 002\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(np.swapaxes(torch.load(os.path.join(root, 'sub-IXI002 - T2.pt')), 0, 1),\n",
        "           cmap='gray', origin='lower')\n",
        "plt.title(\"T2 slice for subject 002\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDkHC_uc99ib"
      },
      "source": [
        "Let's import all the necessary packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "i5N7TFSV99ib"
      },
      "outputs": [],
      "source": [
        "# torch stuff\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# torchsummary and torchvision\n",
        "from torchsummary import summary\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "# matplotlib stuff\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as img\n",
        "\n",
        "# numpy and pandas\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Common python packages\n",
        "import datetime\n",
        "import os\n",
        "import sys\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Kphlt3K99ib"
      },
      "source": [
        "Let's create a custom `IXIDataset` class to easily have access to the data.\n",
        "Here we don't use tsv files to split subjects between the training and the\n",
        "test set. We only set the dataset to the `train` or `test` mode to access\n",
        "training or test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "BszA0ahh99ib"
      },
      "outputs": [],
      "source": [
        "class IXIDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Dataset utility class.\n",
        "\n",
        "    Args:\n",
        "        root: (str) Path of the folder with all the images.\n",
        "        mode : {'train' or 'test'} Part of the dataset that is loaded.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, root, mode=\"train\"):\n",
        "\n",
        "        files = sorted(os.listdir(root))\n",
        "        patient_id = list(set([i.split()[0] for i in files]))\n",
        "\n",
        "        imgs = []\n",
        "\n",
        "        if mode == \"train\":\n",
        "            for i in patient_id[:int(0.8 * len(patient_id))]:\n",
        "                if (\n",
        "                    os.path.isfile(os.path.join(root, i + \" - T1.pt\")) and\n",
        "                    os.path.isfile(os.path.join(root, i + \" - T2.pt\"))\n",
        "                ):\n",
        "                    imgs.append((os.path.join(root, i + \" - T1.pt\"),\n",
        "                                 os.path.join(root, i + \" - T2.pt\")))\n",
        "\n",
        "        elif mode == \"test\":\n",
        "            for i in patient_id[int(0.8 * len(patient_id)):]:\n",
        "                if (\n",
        "                    os.path.isfile(os.path.join(root, i + \" - T1.pt\")) and\n",
        "                    os.path.isfile(os.path.join(root, i + \" - T2.pt\"))\n",
        "                ):\n",
        "                    imgs.append((os.path.join(root, i + \" - T1.pt\"),\n",
        "                                 os.path.join(root, i + \" - T2.pt\")))\n",
        "\n",
        "        self.imgs = imgs\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        t1_path, t2_path = self.imgs[index]\n",
        "\n",
        "        t1 = torch.load(t1_path)[None, :, :]\n",
        "        t2 = torch.load(t2_path)[None, :, :]\n",
        "\n",
        "        return {\"T1\": t1, \"T2\": t2}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dt625lfS99ib"
      },
      "source": [
        "Using this class and the `DataLoader` class from `torch.utils.data`, you can\n",
        "easily have access to your dataset. Here is a quick example on how to use it:\n",
        "\n",
        "```python\n",
        "# Create a DataLoader instance for the training set\n",
        "# You will get a batch of samples from the training set\n",
        "dataloader = DataLoader(\n",
        "    IXIDataset(root, mode=\"train\"),\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "for batch in dataloader:\n",
        "    # batch is a dictionary with two keys:\n",
        "    # - batch[\"T1\"] is a tensor with shape (batch_size, 64, 64) with the T1 images for the samples in this batch\n",
        "    # - batch[\"T2\"] is a tensor with shape (batch_size, 64, 64) with the T2 images for the samples in this batch\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f72xPDGO99ib"
      },
      "source": [
        "# 1. Generator\n",
        "\n",
        "## 1.1 Architecture\n",
        "\n",
        "The generator will have a **U-Net architecture** with the following\n",
        "characteristics:\n",
        "\n",
        "* the descending blocks are convolutional layers followed by instance\n",
        "  normalization with a LeakyReLU activation function;\n",
        "\n",
        "* the ascending blocks are transposed convolutional layers followed by\n",
        "  instance normalization with a ReLU activation function.\n",
        "\n",
        "The parameters for each layer are given in the picture below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-4KjDgK99ib"
      },
      "source": [
        "<a href=\"https://ibb.co/QXBDNy3\">\n",
        "    <img src=\"https://i.ibb.co/g614TkL/Capture-d-cran-2020-03-02-16-04-06.png\" width=\"800\" border=\"0\">\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHw9hDdz99ib"
      },
      "source": [
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>Exercise</b>: Create a <code>GeneratorUNet</code> class to define the\n",
        "generator with the architecture given above.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "tZijZAbs99ic"
      },
      "outputs": [],
      "source": [
        "# We provide classes for each block of the U-Net.\n",
        "\n",
        "class UNetDown(nn.Module):\n",
        "    \"\"\"Descending block of the U-Net.\n",
        "\n",
        "    Args:\n",
        "        in_size: (int) number of channels in the input image.\n",
        "        out_size : (int) number of channels in the output image.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, in_size, out_size):\n",
        "        super(UNetDown, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(in_size, out_size, kernel_size=3, stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(out_size),\n",
        "            nn.LeakyReLU(0.2)\n",
        "          )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class UNetUp(nn.Module):\n",
        "    \"\"\"Ascending block of the U-Net.\n",
        "\n",
        "    Args:\n",
        "        in_size: (int) number of channels in the input image.\n",
        "        out_size : (int) number of channels in the output image.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, in_size, out_size):\n",
        "        super(UNetUp, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_size, out_size, kernel_size=4,\n",
        "                               stride=2, padding=1),\n",
        "            nn.InstanceNorm2d(out_size),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, skip_input=None):\n",
        "        if skip_input is not None:\n",
        "            x = torch.cat((x, skip_input), 1)  # add the skip connection\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FinalLayer(nn.Module):\n",
        "    \"\"\"Final block of the U-Net.\n",
        "\n",
        "    Args:\n",
        "        in_size: (int) number of channels in the input image.\n",
        "        out_size : (int) number of channels in the output image.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, in_size, out_size):\n",
        "        super(FinalLayer, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(in_size, out_size, kernel_size=3, padding=1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, skip_input=None):\n",
        "        if skip_input is not None:\n",
        "            x = torch.cat((x, skip_input), 1)  # add the skip connection\n",
        "        x = self.model(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZYCdIhM99ic",
        "outputId": "678de4b9-c8c9-4059-eae6-a13c06c132da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (<ipython-input-6-7b8d0b16c0ec>, line 7)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-7b8d0b16c0ec>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    # To complete\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ],
      "source": [
        "class GeneratorUNet(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=1):\n",
        "        super(GeneratorUNet, self).__init__()\n",
        "        # To complete\n",
        "\n",
        "    def forward(self, x):\n",
        "        # To complete"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWNxBe_U99ic"
      },
      "source": [
        "Let's have a look at the architecture of our generator:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "ek0LJWPs99ic",
        "outputId": "129798b5-4769-484a-f531-9c8f26841e80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'GeneratorUNet' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-168110e0ff26>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Summary of the generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGeneratorUNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'GeneratorUNet' is not defined"
          ]
        }
      ],
      "source": [
        "# Summary of the generator\n",
        "summary(GeneratorUNet().cuda(), (1, 64, 64) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmPS3jES99ic"
      },
      "source": [
        "## 1.2 Train the generator\n",
        "\n",
        "In order to train the generator, we will repeat the following process:\n",
        "\n",
        "1. Generate T2-w images from T1-w images.\n",
        "2. Compute the error between the true T2-w images and the generated T2-w images.\n",
        "3. Update the parameters of the generators.\n",
        "\n",
        "The training phase looks like this:\n",
        "\n",
        "```\n",
        "# For each epoch\n",
        "\n",
        "    # For each batch\n",
        "\n",
        "        # Generate fake images for all the images in this batch\n",
        "\n",
        "        # Compute the loss for the generator\n",
        "\n",
        "        # Perform one optimization step\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sw5TBjIO99ic"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>Exercise</b>: We provide below a template to train our generator\n",
        " on the dataset. Fill in the missing parts.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "VA921fhP99ic"
      },
      "outputs": [],
      "source": [
        "def train_generator(train_loader, test_loader, num_epoch=500,\n",
        "                    lr=0.0001, beta1=0.9, beta2=0.999):\n",
        "    \"\"\"Train a generator on its own.\n",
        "\n",
        "    Args:\n",
        "        train_loader: (DataLoader) a DataLoader wrapping the training dataset\n",
        "        test_loader: (DataLoader) a DataLoader wrapping the test dataset\n",
        "        num_epoch: (int) number of epochs performed during training\n",
        "        lr: (float) learning rate of the discriminator and generator Adam optimizers\n",
        "        beta1: (float) beta1 coefficient of the discriminator and generator Adam optimizers\n",
        "        beta2: (float) beta1 coefficient of the discriminator and generator Adam optimizers\n",
        "\n",
        "    Returns:\n",
        "        generator: (nn.Module) the trained generator\n",
        "    \"\"\"\n",
        "\n",
        "    cuda = True if torch.cuda.is_available() else False\n",
        "    print(f\"Using cuda device: {cuda}\")  # check if GPU is used\n",
        "\n",
        "    # Tensor type (put everything on GPU if possible)\n",
        "    Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "\n",
        "    # Output folder\n",
        "    if not os.path.exists(\"./images/generator\"):\n",
        "        os.makedirs(\"./images/generator\")\n",
        "\n",
        "    # Loss function\n",
        "    criterion =    # To complete. A loss for a voxel-wise comparison of images like torch.nn.L1Loss\n",
        "\n",
        "    # Initialize the generator\n",
        "    generator =    # To complete.\n",
        "\n",
        "    if cuda:\n",
        "        generator = generator.cuda()\n",
        "        criterion.cuda()\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.Adam(generator.parameters(),\n",
        "                                 lr=lr, betas=(beta1, beta2))\n",
        "\n",
        "    def sample_images(epoch):\n",
        "        \"\"\"Saves a generated sample from the validation set\"\"\"\n",
        "        imgs = next(iter(test_loader))\n",
        "        real_A = imgs[\"T1\"].type(Tensor)\n",
        "        real_B = imgs[\"T2\"].type(Tensor)\n",
        "        fake_B = generator(real_A)\n",
        "        img_sample = torch.cat((real_A.data, fake_B.data, real_B.data), -2)\n",
        "        save_image(img_sample, f\"./images/generator/epoch-{epoch}.png\",\n",
        "                   nrow=5, normalize=True)\n",
        "\n",
        "    # ----------\n",
        "    #  Training\n",
        "    # ----------\n",
        "\n",
        "    prev_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epoch):\n",
        "        for i, batch in enumerate(train_loader):\n",
        "\n",
        "            # Inputs T1-w and T2-w\n",
        "            real_t1 = batch[\"T1\"].type(Tensor)\n",
        "            real_t2 = batch[\"T2\"].type(Tensor)\n",
        "\n",
        "            # Remove stored gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Generate fake T2 images from the true T1 images\n",
        "            fake_t2 =    # To complete\n",
        "\n",
        "            # Compute the corresponding loss\n",
        "            loss =     # To complete\n",
        "\n",
        "            # Compute the gradient and perform one optimization step\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # --------------\n",
        "            #  Log Progress\n",
        "            # --------------\n",
        "\n",
        "            # Determine approximate time left\n",
        "            batches_done = epoch * len(train_loader) + i\n",
        "            batches_left = num_epoch * len(train_loader) - batches_done\n",
        "            time_left = datetime.timedelta(\n",
        "                seconds=batches_left * (time.time() - prev_time))\n",
        "            prev_time = time.time()\n",
        "\n",
        "            # Print log\n",
        "            sys.stdout.write(\n",
        "                \"\\r[Epoch %d/%d] [Batch %d/%d] [Loss: %f] ETA: %s\"\n",
        "                % (\n",
        "                    epoch + 1,\n",
        "                    num_epoch,\n",
        "                    i,\n",
        "                    len(train_loader),\n",
        "                    loss.item(),\n",
        "                    time_left,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Save images at the end of each epoch\n",
        "        sample_images(epoch)\n",
        "\n",
        "    return generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "remove_output"
        ],
        "id": "idZgfmXj99ic"
      },
      "outputs": [],
      "source": [
        "# Parameters for Adam optimizer\n",
        "lr = 0.0002\n",
        "beta1 = 0.5\n",
        "beta2 = 0.999\n",
        "\n",
        "# Create dataloaders\n",
        "batch_size = 40\n",
        "train_loader = DataLoader(IXIDataset(root, mode=\"train\"),\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(IXIDataset(root, mode=\"test\"),\n",
        "                         batch_size=5,\n",
        "                         shuffle=False)\n",
        "\n",
        "# Number of epochs\n",
        "num_epoch = 20\n",
        "\n",
        "# Train the generator\n",
        "generator = train_generator(train_loader, test_loader, num_epoch=num_epoch,\n",
        "                            lr=lr, beta1=beta1, beta2=beta2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epGywZHb99ic"
      },
      "source": [
        "## 1.3 Evaluate the generator\n",
        "\n",
        "Let's visualize a few generated T2-weighted images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "wgKnpMN599ic"
      },
      "outputs": [],
      "source": [
        "im = img.imread(f'./images/generator/epoch-{num_epoch - 1}.png')\n",
        "plt.figure(figsize=(20, 20))\n",
        "plt.imshow(np.swapaxes(im, 0, 1))\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9birkV7X99ic"
      },
      "source": [
        "After doing visual quality control, it is a good idea to quantify the quality\n",
        "of the generated images using specific metrics. Some popular metrics include\n",
        "the Mean Absolute Error (MAE), the Peak Signal-to-Noise Ratio (PSNR) and\n",
        "the Structural Similarity index (SSIM):\n",
        "\n",
        "* MAE = $\\displaystyle \\frac{1}{nm} \\sum_{i=1}^n \\sum_{j=1}^m \\vert T_{ij} - G_{ij} \\vert $\n",
        "\n",
        "* PSNR = $\\displaystyle -10 \\log_{10} \\left( \\frac{1}{nm} \\sum_{i=1}^n \\sum_{j=1}^m (T_{ij} - G_{ij})^2 \\right) $\n",
        "\n",
        "* SSIM = $\\displaystyle  \\frac{(2 \\mu_T \\mu_G + C_1)(2 \\sigma_{TG} + C_2)}{(\\mu_T^2 +\n",
        "\\mu_G^2 + C_1)(\\sigma_T^2 + \\sigma_G^2 + C_2)} $ where:\n",
        "\n",
        "    * $\\mu$ and $\\sigma$ are the mean value and standard deviation of an image respectively, and\n",
        "    * $C_1$ and $C_2$ are two positive constants (one can take $C_1=0.01$ and $C_2=0.03$).\n",
        "\n",
        "The [mean absolute error](https://en.wikipedia.org/wiki/Mean_absolute_error)\n",
        "is simply the mean of each absolute value of the difference between\n",
        "the true pixel ($T_{ij}$) and the generated pixel ($G_{ij}$).\n",
        "The lower, the better. Minimum value is 0.\n",
        "\n",
        "The [peak signal-to-noise ratio](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio)\n",
        "is a function of the mean squared error and allows for comparing\n",
        "images encoded with different scales. We simplified its formula in our case.\n",
        "The higher, the better. Maximum value is $+\\infty$.\n",
        "\n",
        "The [structural similarity index](https://en.wikipedia.org/wiki/Structural_similarity)\n",
        "is a weighted combination of the luminance, the contrast and the structure.\n",
        "The higher, the better. Maximum value is 1.\n",
        "\n",
        "For those interested, you can find [here](https://www.pyimagesearch.com/2014/09/15/python-compare-two-images/)\n",
        "a reference to better understand the differences between these metrics.\n",
        "\n",
        "We provide an implementation for each metric with the functions below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "wNIHkRcx99ic"
      },
      "outputs": [],
      "source": [
        "def mean_absolute_error(image_true, image_generated):\n",
        "    \"\"\"Compute mean absolute error.\n",
        "\n",
        "    Args:\n",
        "        image_true: (Tensor) true image\n",
        "        image_generated: (Tensor) generated image\n",
        "\n",
        "    Returns:\n",
        "        mse: (float) mean squared error\n",
        "    \"\"\"\n",
        "    return torch.abs(image_true - image_generated).mean()\n",
        "\n",
        "\n",
        "def peak_signal_to_noise_ratio(image_true, image_generated):\n",
        "    \"\"\"\"Compute peak signal-to-noise ratio.\n",
        "\n",
        "    Args:\n",
        "        image_true: (Tensor) true image\n",
        "        image_generated: (Tensor) generated image\n",
        "\n",
        "    Returns:\n",
        "        psnr: (float) peak signal-to-noise ratio\"\"\"\n",
        "    mse = ((image_true - image_generated) ** 2).mean().cpu()\n",
        "    return -10 * np.log10(mse)\n",
        "\n",
        "\n",
        "def structural_similarity_index(image_true, image_generated, C1=0.01, C2=0.03):\n",
        "    \"\"\"Compute structural similarity index.\n",
        "\n",
        "    Args:\n",
        "        image_true: (Tensor) true image\n",
        "        image_generated: (Tensor) generated image\n",
        "        C1: (float) variable to stabilize the denominator\n",
        "        C2: (float) variable to stabilize the denominator\n",
        "\n",
        "    Returns:\n",
        "        ssim: (float) mean squared error\"\"\"\n",
        "    mean_true = image_true.mean()\n",
        "    mean_generated = image_generated.mean()\n",
        "    std_true = image_true.std()\n",
        "    std_generated = image_generated.std()\n",
        "    covariance = (\n",
        "        (image_true - mean_true) * (image_generated - mean_generated)).mean()\n",
        "\n",
        "    numerator = (2 * mean_true * mean_generated + C1) * (2 * covariance + C2)\n",
        "    denominator = ((mean_true ** 2 + mean_generated ** 2 + C1) *\n",
        "                   (std_true ** 2 + std_generated ** 2 + C2))\n",
        "    return numerator / denominator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6v7bxFL99ic"
      },
      "source": [
        "We will now evaluate the generator with these three metrics on both the\n",
        "training set and the test set by computing the mean value for each metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "yFjHp4OK99id"
      },
      "outputs": [],
      "source": [
        "def evaluate_generator(generator):\n",
        "    \"\"\"Evaluate a generator.\n",
        "\n",
        "    Args:\n",
        "        generator: (GeneratorUNet) neural network generating T2-w images\n",
        "\n",
        "    \"\"\"\n",
        "    res_train, res_test = [], []\n",
        "\n",
        "    cuda = True if torch.cuda.is_available() else False\n",
        "    Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for i, batch in enumerate(train_loader):\n",
        "\n",
        "            # Inputs T1-w and T2-w\n",
        "            real_t1 = batch[\"T1\"].type(Tensor)\n",
        "            real_t2 = batch[\"T2\"].type(Tensor)\n",
        "            fake_t2 = generator(real_t1)\n",
        "\n",
        "            mae = mean_absolute_error(real_t2, fake_t2).item()\n",
        "            psnr = peak_signal_to_noise_ratio(real_t2, fake_t2).item()\n",
        "            ssim = structural_similarity_index(real_t2, fake_t2).item()\n",
        "\n",
        "            res_train.append([mae, psnr, ssim])\n",
        "\n",
        "        for i, batch in enumerate(test_loader):\n",
        "\n",
        "            # Inputs T1-w and T2-w\n",
        "            real_t1 = batch[\"T1\"].type(Tensor)\n",
        "            real_t2 = batch[\"T2\"].type(Tensor)\n",
        "            fake_t2 = generator(real_t1)\n",
        "\n",
        "            mae = mean_absolute_error(real_t2, fake_t2).item()\n",
        "            psnr = peak_signal_to_noise_ratio(real_t2, fake_t2).item()\n",
        "            ssim = structural_similarity_index(real_t2, fake_t2).item()\n",
        "\n",
        "            res_test.append([mae, psnr, ssim])\n",
        "\n",
        "    df = pd.DataFrame([\n",
        "        pd.DataFrame(res_train, columns=['MAE', 'PSNR', 'SSIM']).mean().squeeze(),\n",
        "        pd.DataFrame(res_test, columns=['MAE', 'PSNR', 'SSIM']).mean().squeeze()\n",
        "    ], index=['Training set', 'Test set']).T\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "ujUstPxz99id"
      },
      "outputs": [],
      "source": [
        "evaluate_generator(generator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJK-EA-099id"
      },
      "source": [
        "The performance is already really good! The task may be pretty easy.\n",
        "Let's see if we can still improve the performance with a more complex neural\n",
        "network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GU5eLPsW99id"
      },
      "source": [
        "# 2. Conditional Generative Adversarial Network (cGAN)\n",
        "\n",
        "A generative adversarial network (GAN) is a network generating new samples.\n",
        "A typical GAN consists of two networks:\n",
        "\n",
        "* a **generator** that generates new samples, and\n",
        "* a **discriminator** that discriminate generated samples from true samples.\n",
        "\n",
        "One can think of the generator as a *counterfeiter* and the discriminator\n",
        "as a *authenticator*.\n",
        "The discriminator aims at improving the generator by having an opposition.\n",
        "The discriminator must not be too good, otherwise the generator won't improve.\n",
        "The generator and the discriminator are trained simultaneously and help\n",
        "each other improve.\n",
        "\n",
        "A conditional generative adversarial network (cGAN) is a particular case\n",
        "of a GAN that is conditioned by the true sample.\n",
        "A conditional GAN can thus only be used when **paired samples** are available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXYJAXxs99id"
      },
      "source": [
        "## 2.1 Architecture of the cGAN\n",
        "\n",
        "Like a GAN, a cGAN has two networks:\n",
        "\n",
        "* a **generator** that generates new samples, and\n",
        "\n",
        "* a **discriminator** that discriminate generated samples from true samples.\n",
        "\n",
        "We will keep the same architecture for the generator.\n",
        "\n",
        "For the discriminator we will use a **two-dimensional convolutional neural\n",
        "network** with 5 layers:\n",
        "\n",
        "* the first 4 layers are 2D-convolutional layers with  a LeakyReLU activation\n",
        "function;\n",
        "\n",
        "* the last layer is a 2D-convolutional layer.\n",
        "\n",
        "The parameters for each layer are given in the figure below. Don't forget\n",
        "that the input of the discriminator will be the generated image and the true\n",
        "image since we are using a conditional GAN. Therefore, the number of input\n",
        "channels for the first layer will be two (one for each image)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4dKxTZn99id"
      },
      "source": [
        "<a href=\"https://ibb.co/9b2jF0V\">\n",
        "  <img src=\"https://i.ibb.co/hBHvPNZ/Capture-d-cran-2020-03-02-16-04-14.png\" width=\"800\"  border=\"0\">\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPK-dKZa99id"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        " <b>Exercise</b>: Create a <code>Discriminator</code> class to define the\n",
        " discriminator with the architecture given above.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "k7vLmHGM99id"
      },
      "outputs": [],
      "source": [
        "# We provide a function to generate a block for the given architecture.\n",
        "def discriminator_block(in_filters, out_filters):\n",
        "    \"\"\"Return downsampling layers of each discriminator block\"\"\"\n",
        "    layers = [nn.Conv2d(in_filters, out_filters, 3, stride=2, padding=1)]\n",
        "    layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "    return layers\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_channels=1):\n",
        "        super(Discriminator, self).__init__()\n",
        "        # To complete\n",
        "\n",
        "\n",
        "    def forward(self, img_A, img_B):\n",
        "        # Concatenate image and condition image by channels to produce input\n",
        "        # To complete"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMI-aFoB99id"
      },
      "source": [
        "Let's have a look at the architecture of our discriminator:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "32gnDqLe99id"
      },
      "outputs": [],
      "source": [
        "# Summary of the discriminator\n",
        "summary(Discriminator().cuda(), [(1, 64, 64), (1, 64, 64)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyxNGlfd99id"
      },
      "source": [
        "## 2.2 Training our conditional GAN\n",
        "\n",
        "Now that we have created our generator and our discriminator, we have to\n",
        "train them on the dataset.\n",
        "\n",
        "**Notations**\n",
        "\n",
        "* $X_{T1}$: true T1 image;\n",
        "* $X_{T2}$: true T2 image;\n",
        "* $\\tilde{X}_{T2}$: generated T2 image from $X_{T1}$;\n",
        "* $\\hat{y}_{X}$: probability returned by the discriminator that the ${X}_{T2}$ is real;\n",
        "* $\\hat{y}_{\\tilde{X}}$: probability returned by the discriminator that the $\\tilde{X}_{T2}$ is real.\n",
        "\n",
        "**Training the generator**\n",
        "\n",
        "The loss for the generator is the sum of:\n",
        "\n",
        "* the binary cross-entropy loss between the predicted probabilities of the\n",
        "generated images and positive labels,\n",
        "* the pixel-wise mean absolute error between the generated image and the true\n",
        "image.\n",
        "\n",
        "For one sample, it is then:\n",
        "\n",
        "$$\n",
        "\\ell_G = - \\log(\\hat{y}_{\\tilde{X}}) + \\lambda * \\text{MAE}(X_{T2}, \\tilde{X}_{T2})\n",
        "$$\n",
        "\n",
        "**Training the discriminator**\n",
        "\n",
        "The loss for the generator is the mean of:\n",
        "\n",
        "* the binary cross-entropy loss between the predicted probabilities of the\n",
        "generated images and negative labels,\n",
        "* the binary cross-entropy loss between the predicted probabilities\n",
        "of the true images and positive labels.\n",
        "\n",
        "For one sample, it is then:\n",
        "\n",
        "$$\n",
        "\\ell_D = - 0.5 * \\log(\\hat{y}_{X}) - 0.5 * \\log(1 - \\hat{y}_{\\tilde{X}})\n",
        "$$\n",
        "\n",
        "**Training phase**\n",
        "\n",
        "The generator and the discriminator are trained simultaneously, which makes\n",
        "the training phase look like this:\n",
        "\n",
        "```\n",
        "# For each epoch\n",
        "\n",
        "    # For each batch\n",
        "\n",
        "        # Generate fake images for all the images in this batch\n",
        "\n",
        "        # Compute the loss for the generator and perform one optimization step\n",
        "\n",
        "        # Compute the loss for the discriminator and perform one optimization step\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNlq3CCW99id"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>Exercise</b>: We provide below a template to train our conditional GAN\n",
        " on the dataset. Fill in the missing parts.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "uKQnq6o-99ig"
      },
      "outputs": [],
      "source": [
        "def train_cgan(train_loader, test_loader, num_epoch=500,\n",
        "               lr=0.0001, beta1=0.9, beta2=0.999):\n",
        "    \"\"\"Train a conditional GAN.\n",
        "\n",
        "    Args:\n",
        "        train_loader: (DataLoader) a DataLoader wrapping a the training dataset\n",
        "        test_loader: (DataLoader) a DataLoader wrapping a the test dataset\n",
        "        num_epoch: (int) number of epochs performed during training\n",
        "        lr: (float) learning rate of the discriminator and generator Adam optimizers\n",
        "        beta1: (float) beta1 coefficient of the discriminator and generator Adam optimizers\n",
        "        beta2: (float) beta1 coefficient of the discriminator and generator Adam optimizers\n",
        "\n",
        "    Returns:\n",
        "        generator: (nn.Module) the trained generator\n",
        "    \"\"\"\n",
        "\n",
        "    cuda = True if torch.cuda.is_available() else False\n",
        "    print(f\"Using cuda device: {cuda}\")  # check if GPU is used\n",
        "\n",
        "    # Tensor type (put everything on GPU if possible)\n",
        "    Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "\n",
        "    # Output folder\n",
        "    if not os.path.exists(\"./images/cgan\"):\n",
        "        os.makedirs(\"./images/cgan\")\n",
        "\n",
        "    # Loss functions\n",
        "    criterion_GAN =    # To complete. A loss adapted to binary classification like torch.nn.BCEWithLogitsLoss\n",
        "    criterion_pixelwise =     # To complete. A loss for a voxel-wise comparison of images like torch.nn.L1Loss\n",
        "\n",
        "    lambda_GAN = 1.  # Weights criterion_GAN in the generator loss\n",
        "    lambda_pixel = 1.  # Weights criterion_pixelwise in the generator loss\n",
        "\n",
        "    # Initialize generator and discriminator\n",
        "    generator =    # To complete\n",
        "    discriminator =    # To complete\n",
        "\n",
        "    if cuda:\n",
        "        generator = generator.cuda()\n",
        "        discriminator = discriminator.cuda()\n",
        "        criterion_GAN.cuda()\n",
        "        criterion_pixelwise.cuda()\n",
        "\n",
        "    # Optimizers\n",
        "    optimizer_generator = torch.optim.Adam(\n",
        "        generator.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "    optimizer_discriminator = torch.optim.Adam(\n",
        "        discriminator.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "\n",
        "    def sample_images(epoch):\n",
        "        \"\"\"Saves a generated sample from the validation set\"\"\"\n",
        "        imgs = next(iter(test_loader))\n",
        "        real_t1 = imgs[\"T1\"].type(Tensor)\n",
        "        real_t2 = imgs[\"T2\"].type(Tensor)\n",
        "        fake_t2 = generator(real_t1)\n",
        "        img_sample = torch.cat((real_t1.data, fake_t2.data, real_t2.data), -2)\n",
        "        save_image(img_sample, f\"./images/cgan/epoch-{epoch}.png\",\n",
        "                   nrow=5, normalize=True)\n",
        "\n",
        "    # ----------\n",
        "    #  Training\n",
        "    # ----------\n",
        "\n",
        "    prev_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epoch):\n",
        "        for i, batch in enumerate(train_loader):\n",
        "\n",
        "            # Inputs T1-w and T2-w\n",
        "            real_t1 = batch[\"T1\"].type(Tensor)\n",
        "            real_t2 = batch[\"T2\"].type(Tensor)\n",
        "\n",
        "            # Create labels\n",
        "            valid = Tensor(np.ones((real_t2.size(0), 1, 1, 1)))\n",
        "            fake = Tensor(np.zeros((real_t2.size(0), 1, 1, 1)))\n",
        "\n",
        "            # -----------------\n",
        "            #  Train Generator\n",
        "            # -----------------\n",
        "            optimizer_generator.zero_grad()\n",
        "\n",
        "            # GAN loss\n",
        "            fake_t2 =    # To complete\n",
        "            pred_fake =    # To complete\n",
        "            loss_GAN =    # To complete\n",
        "\n",
        "            # L1 loss\n",
        "            loss_pixel =    # To complete\n",
        "\n",
        "            # Total loss\n",
        "            loss_generator = lambda_GAN * loss_GAN + lambda_pixel * loss_pixel\n",
        "\n",
        "            # Compute the gradient and perform one optimization step\n",
        "            loss_generator.backward()\n",
        "            optimizer_generator.step()\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            optimizer_discriminator.zero_grad()\n",
        "\n",
        "            # Real loss\n",
        "            pred_real =    # To complete\n",
        "            loss_real =    # To complete\n",
        "\n",
        "            # Fake loss\n",
        "            pred_fake =    # To complete\n",
        "            loss_fake =    # To complete\n",
        "\n",
        "            # Total loss\n",
        "            loss_discriminator = 0.5 * (loss_real + loss_fake)\n",
        "\n",
        "            # Compute the gradient and perform one optimization step\n",
        "            loss_discriminator.backward()\n",
        "            optimizer_discriminator.step()\n",
        "\n",
        "            # --------------\n",
        "            #  Log Progress\n",
        "            # --------------\n",
        "\n",
        "            # Determine approximate time left\n",
        "            batches_done = epoch * len(train_loader) + i\n",
        "            batches_left = num_epoch * len(train_loader) - batches_done\n",
        "            time_left = datetime.timedelta(\n",
        "                seconds=batches_left * (time.time() - prev_time))\n",
        "            prev_time = time.time()\n",
        "\n",
        "            # Print log\n",
        "            sys.stdout.write(\n",
        "                \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] \"\n",
        "                \"[G loss: %f, pixel: %f, adv: %f] ETA: %s\"\n",
        "                % (\n",
        "                    epoch + 1,\n",
        "                    num_epoch,\n",
        "                    i,\n",
        "                    len(train_loader),\n",
        "                    loss_discriminator.item(),\n",
        "                    loss_generator.item(),\n",
        "                    loss_pixel.item(),\n",
        "                    loss_GAN.item(),\n",
        "                    time_left,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Save images at the end of each epoch\n",
        "        sample_images(epoch)\n",
        "\n",
        "    return generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "remove_output"
        ],
        "id": "d4Zo6ubI99ih"
      },
      "outputs": [],
      "source": [
        "generator_cgan = train_cgan(train_loader, test_loader, num_epoch=num_epoch,\n",
        "                            lr=lr, beta1=beta1, beta2=beta2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7e9bs9D99ih"
      },
      "source": [
        "## 2.3 Evaluating the generator of our cGAN\n",
        "\n",
        "Let's visualize a few generated T2-weighted images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjeubiES99ih"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 20))\n",
        "im = img.imread(f'./images/cgan/epoch-{num_epoch - 1}.png')\n",
        "plt.imshow(np.swapaxes(im, 0, 1))\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3Oyk-iQ99ih"
      },
      "source": [
        "We will now evaluate the generator of the cGAN with the same three metrics\n",
        "on both the training set and the test set by computing the mean value for\n",
        "each metric:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "-2LLQWgn99ih"
      },
      "outputs": [],
      "source": [
        "evaluate_generator(generator_cgan)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y48day4U99ih"
      },
      "source": [
        "The performance is slightly lower for the cGAN, which is a bit disappointing.\n",
        "This may be explained by the ease of the task and increase the complexity of\n",
        "the architecture does not help. Another possibility would be to increase the\n",
        "number of epochs, which we did not change in comparison to the architecture\n",
        "with the generator alone.\n",
        "\n",
        "Let's now try a more general approach that does not require paired samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkqMC5-J99ih"
      },
      "source": [
        "# 3. CycleGAN\n",
        "\n",
        "A cycle generative adversarial network (CycleGAN) is a technique for training\n",
        "**unsupervised image translation** models via the GAN architecture using\n",
        "unpaired collections of images from two different domains.\n",
        "The main innovation of a CycleGAN is to introduce a\n",
        "**cycle consistency loss** to enforce good reconstruction in both domains.\n",
        "\n",
        "A CycleGAN consists of two GAN:\n",
        "\n",
        "* one generating samples from domain *A* to domain *B*, and\n",
        "\n",
        "* another one generating from domain *B* to domain *A*.\n",
        "\n",
        "The cycle consistency consists in generating from one domain to the other\n",
        "domain then generating back from the second domain to the first domain, and\n",
        "comparing the generated sample from the original sample.\n",
        "The image below (taken from the [original paper introducing CycleGAN](https://arxiv.org/pdf/1703.10593.pdf))\n",
        "summarizes the main concepts of a CycleGAN:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2Bsysgq99ih"
      },
      "source": [
        "<a href=\"https://ibb.co/WtS49wR\">\n",
        "   <img src=\"https://i.ibb.co/2NX12Qp/cyclegan.png\" width=\"800\" border=\"0\">\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvjHgVit99ih"
      },
      "source": [
        "## 3.1 Architecture of the CycleGAN\n",
        "\n",
        "The discriminators of the CycleGAN do not have the true sample as input.\n",
        "Thus, we have to remove the true sample in the `forward` method.\n",
        "\n",
        "Otherwise, we will use the same architectures for the generators and for\n",
        "the discriminators as the ones from the conditional GAN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "fCuiPof999ih"
      },
      "outputs": [],
      "source": [
        "class DiscriminatorCycle(nn.Module):\n",
        "    def __init__(self, in_channels=1):\n",
        "        super(DiscriminatorCycle, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        layers.extend(discriminator_block(in_channels, 64))\n",
        "        layers.extend(discriminator_block(64, 128))\n",
        "        layers.extend(discriminator_block(128, 256))\n",
        "        layers.extend(discriminator_block(256, 512))\n",
        "        layers.append(nn.Conv2d(512, 1, 4, padding=0))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, img):\n",
        "        return self.model(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wv11Rklq99ih"
      },
      "source": [
        "Let's have a look at the architecture of one discriminator in our CycleGAN:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "7sfgqrRz99ih"
      },
      "outputs": [],
      "source": [
        "# Summary of one discriminator in the CycleGAN\n",
        "summary(DiscriminatorCycle().cuda(), [(1, 64, 64)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqhnvPLb99ih"
      },
      "source": [
        "## 3.2 Training the CycleGAN\n",
        "\n",
        "The generators and the discriminators are trained simultaneously, which makes\n",
        "the training phase look like this:\n",
        "\n",
        "```\n",
        "# For each epoch\n",
        "\n",
        "    # For each batch\n",
        "\n",
        "        # T1 -> T2 -> T1 cycle\n",
        "\n",
        "        ## Generate fake T2-weighted images for all the T1-weighted images in this batch\n",
        "\n",
        "        ## Generate fake T1-weighted images for all the fake generated T2-weighted images in this batch\n",
        "\n",
        "        ## Compute the loss for both generators and perform one optimization step\n",
        "\n",
        "        ## Compute the loss for both discriminators and perform one optimization step\n",
        "\n",
        "        # T2 -> T1 -> T2 cycle\n",
        "\n",
        "        ## Generate fake T1-weighted images for all the T2-weighted images in this batch\n",
        "\n",
        "        ## Generate fake T2-weighted images for all the fake generated T1-weighted images in this batch\n",
        "\n",
        "        ## Compute the loss for both generators and perform one optimization step\n",
        "\n",
        "        ## Compute the loss for both discriminators and perform one optimization step\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbU65QAk99ih"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>Exercise</b>: We provide below a template to train our cycle GAN\n",
        " on the dataset. Fill in the missing parts.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "XUuDzp3A99ih"
      },
      "outputs": [],
      "source": [
        "def train_cyclegan(train_loader, test_loader, num_epoch=500,\n",
        "                   lr=0.0001, beta1=0.9, beta2=0.999):\n",
        "    \"\"\"Train a CycleGAN.\n",
        "\n",
        "    Args:\n",
        "        train_loader: (DataLoader) a DataLoader wrapping a the training dataset\n",
        "        test_loader: (DataLoader) a DataLoader wrapping a the test dataset\n",
        "        num_epoch: (int) number of epochs performed during training\n",
        "        lr: (float) learning rate of the discriminator and generator Adam optimizers\n",
        "        beta1: (float) beta1 coefficient of the discriminator and generator Adam optimizers\n",
        "        beta2: (float) beta1 coefficient of the discriminator and generator Adam optimizers\n",
        "\n",
        "    Returns:\n",
        "        generator: (nn.Module) the generator generating T2-w images from T1-w images.\n",
        "    \"\"\"\n",
        "\n",
        "    cuda = True if torch.cuda.is_available() else False\n",
        "    print(f\"Using cuda device: {cuda}\")  # check if GPU is used\n",
        "\n",
        "    # Tensor type (put everything on GPU if possible)\n",
        "    Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "\n",
        "    # Output folder\n",
        "    if not os.path.exists(\"./images/cyclegan\"):\n",
        "        os.makedirs(\"./images/cyclegan\")\n",
        "\n",
        "    # Loss functions\n",
        "    criterion_GAN_from_t1_to_t2 = torch.nn.BCEWithLogitsLoss()  # A loss adapted to binary classification like torch.nn.BCEWithLogitsLoss\n",
        "    criterion_GAN_from_t2_to_t1 = torch.nn.BCEWithLogitsLoss()  # A loss adapted to binary classification like torch.nn.BCEWithLogitsLoss\n",
        "    criterion_pixelwise_from_t1_to_t2 = torch.nn.L1Loss()  # A loss for a voxel-wise comparison of images like torch.nn.L1Loss\n",
        "    criterion_pixelwise_from_t2_to_t1 = torch.nn.L1Loss()  # A loss for a voxel-wise comparison of images like torch.nn.L1Loss\n",
        "\n",
        "    lambda_GAN = 1.  # Weights criterion_GAN in the generator loss\n",
        "    lambda_pixel = 1.  # Weights criterion_pixelwise in the generator loss\n",
        "\n",
        "    # Initialize generators and discriminators\n",
        "    generator_from_t1_to_t2 =    # To complete\n",
        "    generator_from_t2_to_t1 =    # To complete\n",
        "    discriminator_from_t1_to_t2 =    # To complete\n",
        "    discriminator_from_t2_to_t1 =    # To complete\n",
        "\n",
        "    if cuda:\n",
        "        generator_from_t1_to_t2 = generator_from_t1_to_t2.cuda()\n",
        "        generator_from_t2_to_t1 = generator_from_t2_to_t1.cuda()\n",
        "\n",
        "        discriminator_from_t1_to_t2 = discriminator_from_t1_to_t2.cuda()\n",
        "        discriminator_from_t2_to_t1 = discriminator_from_t2_to_t1.cuda()\n",
        "\n",
        "        criterion_GAN_from_t1_to_t2 = criterion_GAN_from_t1_to_t2.cuda()\n",
        "        criterion_GAN_from_t2_to_t1 = criterion_GAN_from_t2_to_t1.cuda()\n",
        "\n",
        "        criterion_pixelwise_from_t1_to_t2 = criterion_pixelwise_from_t1_to_t2.cuda()\n",
        "        criterion_pixelwise_from_t2_to_t1 = criterion_pixelwise_from_t2_to_t1.cuda()\n",
        "\n",
        "    # Optimizers\n",
        "    optimizer_generator_from_t1_to_t2 = torch.optim.Adam(\n",
        "        generator_from_t1_to_t2.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "    optimizer_generator_from_t2_to_t1 = torch.optim.Adam(\n",
        "        generator_from_t2_to_t1.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "\n",
        "    optimizer_discriminator_from_t1_to_t2 = torch.optim.Adam(\n",
        "        discriminator_from_t1_to_t2.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "    optimizer_discriminator_from_t2_to_t1 = torch.optim.Adam(\n",
        "        discriminator_from_t2_to_t1.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "\n",
        "    def sample_images(epoch):\n",
        "        \"\"\"Saves a generated sample from the validation set\"\"\"\n",
        "        imgs = next(iter(test_loader))\n",
        "        real_t1 = imgs[\"T1\"].type(Tensor)\n",
        "        real_t2 = imgs[\"T2\"].type(Tensor)\n",
        "        fake_t2 = generator_from_t1_to_t2(real_t1)\n",
        "        img_sample = torch.cat((real_t1.data, fake_t2.data, real_t2.data), -2)\n",
        "        save_image(img_sample, f\"./images/cyclegan/epoch-{epoch}.png\",\n",
        "                   nrow=5, normalize=True)\n",
        "\n",
        "    # ----------\n",
        "    #  Training\n",
        "    # ----------\n",
        "\n",
        "    prev_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epoch):\n",
        "        for i, batch in enumerate(train_loader):\n",
        "\n",
        "            # Inputs T1-w and T2-w\n",
        "            real_t1 = batch[\"T1\"].type(Tensor)\n",
        "            real_t2 = batch[\"T2\"].type(Tensor)\n",
        "\n",
        "            # Create labels\n",
        "            valid_t1 = Tensor(np.ones((real_t1.size(0), 1, 1, 1)))\n",
        "            imitation_t1 = Tensor(np.zeros((real_t1.size(0), 1, 1, 1)))\n",
        "\n",
        "            valid_t2 = Tensor(np.ones((real_t2.size(0), 1, 1, 1)))\n",
        "            imitation_t2 = Tensor(np.zeros((real_t2.size(0), 1, 1, 1)))\n",
        "\n",
        "            # ------------------\n",
        "            #  Train Generators\n",
        "            # ------------------\n",
        "            optimizer_generator_from_t1_to_t2.zero_grad()\n",
        "            optimizer_generator_from_t2_to_t1.zero_grad()\n",
        "\n",
        "            # GAN loss\n",
        "            fake_t2 =    # To complete\n",
        "            pred_fake_t2 =    # To complete\n",
        "            loss_GAN_from_t1_to_t2 =    # To complete\n",
        "\n",
        "            fake_t1 =    # To complete\n",
        "            pred_fake_t1 =    # To complete\n",
        "            loss_GAN_from_t2_to_t1 =    # To complete\n",
        "\n",
        "            # L1 loss\n",
        "            fake_fake_t1 =    # To complete\n",
        "            loss_pixel_from_t1_to_t2 =    # To complete\n",
        "\n",
        "            fake_fake_t2 =    # To complete\n",
        "            loss_pixel_from_t2_to_t1 =    # To complete\n",
        "\n",
        "            # Total loss\n",
        "            loss_generator_from_t1_to_t2 = (lambda_GAN * loss_GAN_from_t1_to_t2 +\n",
        "                                            lambda_pixel * loss_pixel_from_t1_to_t2)\n",
        "            loss_generator_from_t2_to_t1 = (lambda_GAN * loss_GAN_from_t2_to_t1 +\n",
        "                                            lambda_pixel * loss_pixel_from_t2_to_t1)\n",
        "\n",
        "            loss_generator_from_t1_to_t2.backward()\n",
        "            loss_generator_from_t2_to_t1.backward()\n",
        "\n",
        "            optimizer_generator_from_t1_to_t2.step()\n",
        "            optimizer_generator_from_t2_to_t1.step()\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            optimizer_discriminator_from_t1_to_t2.zero_grad()\n",
        "            optimizer_discriminator_from_t2_to_t1.zero_grad()\n",
        "\n",
        "            # Real loss\n",
        "            pred_real_t2 =    # To complete\n",
        "            loss_real_t2 =    # To complete\n",
        "\n",
        "            pred_real_t1 =    # To complete\n",
        "            loss_real_t1 =    # To complete\n",
        "\n",
        "            # Fake loss\n",
        "            pred_fake_t2 =    # To complete\n",
        "            loss_fake_t2 =    # To complete\n",
        "\n",
        "            pred_fake_t1 =    # To complete\n",
        "            loss_fake_t1 =    # To complete\n",
        "\n",
        "            # Total loss\n",
        "            loss_discriminator_from_t1_to_t2 = 0.5 * (loss_real_t2 + loss_fake_t2)\n",
        "            loss_discriminator_from_t2_to_t1 = 0.5 * (loss_real_t1 + loss_fake_t1)\n",
        "\n",
        "            loss_discriminator_from_t1_to_t2.backward()\n",
        "            loss_discriminator_from_t2_to_t1.backward()\n",
        "\n",
        "            optimizer_discriminator_from_t1_to_t2.step()\n",
        "            optimizer_discriminator_from_t2_to_t1.step()\n",
        "\n",
        "            # --------------\n",
        "            #  Log Progress\n",
        "            # --------------\n",
        "\n",
        "            # Determine approximate time left\n",
        "            batches_done = epoch * len(train_loader) + i\n",
        "            batches_left = num_epoch * len(train_loader) - batches_done\n",
        "            time_left = datetime.timedelta(\n",
        "                seconds=batches_left * (time.time() - prev_time))\n",
        "            prev_time = time.time()\n",
        "\n",
        "            # Print log\n",
        "            sys.stdout.write(\n",
        "                \"\\r[Epoch %d/%d] [Batch %d/%d] \"\n",
        "                \"[Generator losses: %f, %f] \"\n",
        "                \"[Discriminator losses: %f, %f] \"\n",
        "                \"ETA: %s\"\n",
        "                % (\n",
        "                    epoch + 1,\n",
        "                    num_epoch,\n",
        "                    i,\n",
        "                    len(train_loader),\n",
        "                    loss_generator_from_t1_to_t2.item(),\n",
        "                    loss_generator_from_t2_to_t1.item(),\n",
        "                    loss_discriminator_from_t1_to_t2.item(),\n",
        "                    loss_discriminator_from_t2_to_t1.item(),\n",
        "                    time_left,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Save images at the end of each epoch\n",
        "        sample_images(epoch)\n",
        "\n",
        "    return generator_from_t1_to_t2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "remove_output"
        ],
        "id": "wLkABQu_99ii"
      },
      "outputs": [],
      "source": [
        "generator_cyclegan = train_cyclegan(\n",
        "    train_loader, test_loader, num_epoch=num_epoch,\n",
        "    lr=lr, beta1=beta1, beta2=beta2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTDBLXzp99ii"
      },
      "source": [
        "## 3.3 Evaluating the generator of our CycleGAN\n",
        "\n",
        "Let's visualize a few generated T2-weighted images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EecuQbXu99ii"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 20))\n",
        "im = img.imread(f'./images/cyclegan/epoch-{num_epoch - 1}.png')\n",
        "plt.imshow(np.swapaxes(im, 0, 1))\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVxqpOsX99ii"
      },
      "source": [
        "We will now evaluate the generator of the CycleGAN with the same three\n",
        "metrics on both the training set and the test set by computing the mean value\n",
        "for each metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "620zmWO899ii"
      },
      "outputs": [],
      "source": [
        "evaluate_generator(generator_cyclegan)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaMCwBst99ii"
      },
      "source": [
        "You should obtain a lower performance for the CycleGAN, which is not so\n",
        "surprising since this task is unsupervised whereas the two other tasks are\n",
        "supervised.\n",
        "\n",
        "It does not mean that cycle GAN are not useful in practice.\n",
        "Datasets of unpaired samples are much more common than datasets of paired\n",
        "samples. Here is an example of transforming a picture into a painting from\n",
        "a famous painter using a CycleGAN\n",
        "(taken from the [original paper introducing CycleGAN](https://arxiv.org/pdf/1703.10593.pdf)):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yuACtL699ii"
      },
      "source": [
        "<a href=\"https://ibb.co/Q8FrRrd\">\n",
        "  <img src=\"https://i.ibb.co/6vRgxgB/style-transfer.png\" width=\"800\" border=\"0\">\n",
        "</a>"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_json": true,
      "main_language": "python"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}